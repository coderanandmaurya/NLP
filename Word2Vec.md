# ğŸ“˜ Word2Vec and Word Embedding Techniques

*(From Why We Need Embeddings to Practical Usage)*

---

## 1ï¸âƒ£ Why Do We Need Word Embeddings? (Bridge from TF-IDF)

### âŒ Problem with Previous Techniques

From the previous lecture:

* One-Hot, BoW, TF-IDF
* High-dimensional
* Sparse vectors
* No true semantic meaning

Example:

```
good â†’ [0,1,0,0,0]
excellent â†’ [0,0,0,1,0]
```

ğŸ“Œ Machine thinks **good â‰  excellent**

---

### â“ What Humans Expect

Humans know:

```
good â‰ˆ excellent â‰ˆ awesome
```

â¡ï¸ **Words with similar meaning should have similar numeric representations**

This is where **Word Embeddings** come in.

---

## 2ï¸âƒ£ What Is a Word Embedding? (Very Basic)

### ğŸ“Œ Definition

A **word embedding** is a way of representing words as **dense numeric vectors** such that:

* Similar words â†’ similar vectors
* Related words â†’ close in vector space

Example intuition:

```
king  â†’ [0.21, -0.34, 0.56, ...]
queen â†’ [0.19, -0.30, 0.58, ...]
```

ğŸ“Œ These are **real numbers**, not 0/1.

---

## 3ï¸âƒ£ Sparse vs Dense Vectors (Critical Basic)

### âŒ Sparse Vector (BoW / TF-IDF)

Vocabulary size = 50,000
Sentence length = 10

Vector:

```
[0,0,0,1,0,0,0,0,0,0,...]
```

Problems:

* Memory waste
* Slower computation
* Curse of dimensionality

---

### âœ… Dense Vector (Word2Vec)

Vector size = 100 or 300

```
[0.21, -0.34, 0.56, 0.89, ...]
```

Benefits:

* Compact
* Meaningful
* Efficient

---

## 4ï¸âƒ£ What Is Word2Vec?

### ğŸ“Œ Definition

**Word2Vec** is a **prediction-based word embedding technique** developed by Google (2013â€“2015) that uses **neural networks** to learn semantic word representations.

ğŸ“Œ It improves over:

* BoW
* TF-IDF

By:

* Learning meaning from **context**
* Producing **dense embeddings**

---

## 5ï¸âƒ£ Two Types of Word Representation (Big Picture)

| Category         | Idea          | Examples     |
| ---------------- | ------------- | ------------ |
| Frequency-based  | Count words   | BoW, TF-IDF  |
| Prediction-based | Predict words | **Word2Vec** |

ğŸ“Œ Word2Vec learns by **prediction, not counting**.

---

## 6ï¸âƒ£ Core Idea Behind Word2Vec (Intuition)

> **Words appearing in similar contexts have similar meanings**

Example:

```
I drink coffee
I drink tea
```

ğŸ“Œ coffee and tea appear in **same context**
â¡ï¸ Their vectors become similar

---

## 7ï¸âƒ£ Context Window (Very Basic)

Sentence:

```
I love machine learning very much
```

If window size = 2

For word **machine**, context words:

```
love, learning
```

ğŸ“Œ Word2Vec slides this window across text to learn relationships.

---

## 8ï¸âƒ£ Word2Vec Architectures

### 1ï¸âƒ£ CBOW (Continuous Bag of Words)

ğŸ“Œ Predict **target word** from **context words**

Example:

```
Context: I love ___ learning
Target: machine
```

Characteristics:

* Faster
* Works well on large datasets
* Less accurate for rare words

---

### 2ï¸âƒ£ Skip-Gram

ğŸ“Œ Predict **context words** from **target word**

Example:

```
Target: machine
Predict: love, learning
```

Characteristics:

* Slower
* Better for small datasets
* Better for rare words

---

## 9ï¸âƒ£ Neural Network View (Simple)

Word2Vec uses a **shallow neural network**:

* Input layer â†’ word index
* Hidden layer â†’ embedding (actual word vector)
* Output layer â†’ predicted word(s)

ğŸ“Œ The **hidden layer weights = word embeddings**

---

## ğŸ”Ÿ Why Word2Vec Is Powerful

### âœ… Semantic Similarity

```
happy â‰ˆ joy
king â‰ˆ queen
```

### âœ… Vector Arithmetic

[
king - man + woman \approx queen
]

This shows **semantic understanding**, not memorization.

---

### âœ… Dense & Efficient

* Fewer dimensions (100â€“300)
* Faster training
* Less overfitting

---

## 1ï¸âƒ£1ï¸âƒ£ Training Word2Vec (High-Level Workflow)

| Step                 | Description            |
| -------------------- | ---------------------- |
| Data Collection      | Large corpus           |
| Preprocessing        | Tokenization, cleaning |
| Vocabulary           | Unique words           |
| Windowing            | Define context         |
| Model Training       | Neural prediction      |
| Embedding Extraction | Vectors                |
| Evaluation           | Similarity & analogy   |

---

## 1ï¸âƒ£2ï¸âƒ£ Practical Demonstration (Conceptual)

Using **Gensim**:

* Load pre-trained Google News vectors
* Each word â†’ 300-D vector
* Compute similarity:

```
similarity("man", "woman")
similarity("king", "queen")
```

ğŸ“Œ Vectors are **dense and meaningful**

---

## 1ï¸âƒ£3ï¸âƒ£ Domain Example: Game of Thrones

* Train Word2Vec on GoT scripts
* Character names become embeddings
* Similarity examples:

```
Jon Snow â†” Night Watch
Daenerys â†” Dragon
```

ğŸ“Œ Model learns **domain-specific meaning**

---

## 1ï¸âƒ£4ï¸âƒ£ Improving Word2Vec Models

| Factor             | Effect                  |
| ------------------ | ----------------------- |
| More data          | Better embeddings       |
| Larger vector size | More meaning, more cost |
| Larger window      | Broader context         |
| Bigger vocabulary  | Better coverage         |

---

## 1ï¸âƒ£5ï¸âƒ£ Limitations of Word2Vec (Important)

* Same word â†’ same vector in all contexts

  ```
  bank (river) = bank (money) âŒ
  ```
* Requires large data
* Not contextual

ğŸ“Œ This leads to **GloVe, FastText, BERT**

---

## ğŸ”š Final Takeaway

* Word2Vec converts words into **dense semantic vectors**
* Learns meaning from **context**
* Solves major problems of BoW & TF-IDF
* Foundation of **modern NLP**

---
Just tell me what to do next.

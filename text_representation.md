# ğŸ“˜ Text Representation Techniques in Machine Learning

*(From Absolute Basics to TF-IDF)*

---

## 1ï¸âƒ£ First Principle: What Does a Machine Learning Model Understand?

A machine learning model **does not understand language**.

It only understands **numbers** and performs mathematical operations such as:

[
y = wx + b
]

So when we give text like:

> **â€œI love machine learningâ€**

the model **cannot process it directly**.

ğŸ“Œ Therefore, **text must be converted into numbers**.

This conversion process is called:

* **Text Representation**
* **Text Vectorization**
* **Feature Extraction from Text**

---

## 2ï¸âƒ£ What Is a Feature? (Text Perspective)

In tabular data:

* Columns = **features**
* Rows = **data points**

Example:

| Hours Studied | Marks |
| ------------- | ----- |
| 6             | 70    |

Here, *Hours Studied* is a feature.

---

### â“ In Text Data, What Is the Feature?

Sentence:

```
"I love ML"
```

Words:

```
I, love, ML
```

ğŸ“Œ These words must become **numeric features**.

---

## 3ï¸âƒ£ Tokenization and Vocabulary (Foundation Step)

### ğŸ”¹ Tokenization

Breaking text into words.

```
"I love ML" â†’ ["I", "love", "ML"]
```

Each word is called a **token**.

---

### ğŸ”¹ Vocabulary

The set of **unique tokens** in the dataset.

Dataset:

```
"I love ML"
"I love AI"
```

Vocabulary:

```
["I", "love", "ML", "AI"]
```

ğŸ“Œ Vocabulary size decides **vector dimension**.

---

## 4ï¸âƒ£ Why Fixed-Length Vectors Are Required?

Machine learning models:

* Expect the **same number of inputs**
* Cannot handle variable-length text

Example:

```
"I love ML" â†’ 3 words
"I really love ML" â†’ 4 words
```

âŒ Not acceptable

â¡ï¸ Text representation converts all sentences into **fixed-length numeric vectors**.

---

## 5ï¸âƒ£ Challenges in Text Representation

* Large vocabulary â†’ high dimensional vectors
* **Sparsity** (too many zeros)
* **Out-of-Vocabulary (OOV)** words
* No semantic understanding
* Loss of word order

ğŸ“Œ *Garbage In â†’ Garbage Out* applies strongly here.

---

## 6ï¸âƒ£ One-Hot Encoding

### ğŸ“Œ Concept

Each word is represented by a vector with:

* One `1`
* Remaining `0`s

Vocabulary:

```
["I", "love", "ML"]
```

| Word | Vector    |
| ---- | --------- |
| I    | [1, 0, 0] |
| love | [0, 1, 0] |
| ML   | [0, 0, 1] |

### âŒ Limitations

* Very high dimensional
* Sparse vectors
* No meaning or similarity
* Not scalable

---

## 7ï¸âƒ£ Bag of Words (BoW)

### ğŸ“Œ Concept

* Count word frequency
* Ignore word order

Example sentences:

```
S1: I love ML
S2: I love AI
```

Vocabulary:

```
["I", "love", "ML", "AI"]
```

| Sentence | Vector       |
| -------- | ------------ |
| S1       | [1, 1, 1, 0] |
| S2       | [1, 1, 0, 1] |

### âœ… Advantages

* Simple
* Fixed-size vectors
* Effective for basic classification

### âŒ Limitations

```
"dog bites man"
"man bites dog"
```

â¡ï¸ Same vector, different meaning

---

## 8ï¸âƒ£ Bag of N-Grams

### ğŸ“Œ Motivation

To capture **partial word order**.

Example:

```
"not good"
```

Bigrams preserve negation better than unigrams.

### âŒ Drawback

* Vocabulary size increases rapidly
* Higher memory and computation

---

## 9ï¸âƒ£ TF-IDF (Term Frequency â€“ Inverse Document Frequency)

### ğŸ“Œ Problem with BoW

Common words like:

```
is, the, and
```

appear everywhere and add noise.

---

### ğŸ”¢ Term Frequency (TF)

[
TF = \frac{\text{Word count in document}}{\text{Total words in document}}
]

---

### ğŸ”¢ Inverse Document Frequency (IDF)

[
IDF = \log\left(\frac{\text{Total documents}}{\text{Documents containing the word}}\right) + 1
]

---

### ğŸ”¢ TF-IDF Score

[
TF\text{-}IDF = TF \times IDF
]

ğŸ“Œ Important words receive **higher weight**.

---

### Example Insight

Documents:

```
D1: ML is powerful
D2: ML is fun
```

| Word | TF-IDF Weight |
| ---- | ------------- |
| ML   | High          |
| is   | Low           |

---

## ğŸ”Ÿ Out-of-Vocabulary (OOV) Problem

Training text:

```
"movie is good"
```

Test text:

```
"movie is excellent"
```

ğŸ“Œ **excellent** not in vocabulary â†’ ignored

---

## 1ï¸âƒ£1ï¸âƒ£ Custom / Handcrafted Features

Examples:

* Positive word count
* Negative word count
* Sentence length
* Presence of words like *not*

Hybrid features often improve performance.

---

## 1ï¸âƒ£2ï¸âƒ£ Technique Comparison

| Technique | Word Order | Semantics  | Sparsity  |
| --------- | ---------- | ---------- | --------- |
| One-Hot   | âŒ          | âŒ          | High      |
| BoW       | âŒ          | âŒ          | High      |
| N-Grams   | âš ï¸ Partial | âš ï¸ Partial | Very High |
| TF-IDF    | âŒ          | âš ï¸ Partial | High      |

---

## 1ï¸âƒ£3ï¸âƒ£ Why We Learn These Techniques

* Foundation of NLP
* Easy to interpret
* Fast on small datasets
* Common in interviews
* Basis for Word2Vec & BERT

---

## ğŸ”š Conclusion

Text representation is a **critical step** in any NLP pipeline.
Classical methods like **One-Hot, BoW, N-Grams, and TF-IDF** convert text into numbers but fail to fully capture meaning.

These techniques form the **bridge** to advanced embedding methods, which will be covered next.

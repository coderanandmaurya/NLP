# NLP Pipeline

### Lecture Notes with Practical Examples and Industry Perspective

## 1. Introduction

In real-world AI systems, solving an NLP problem is **not only about applying a machine learning or deep learning model**. Instead, it requires designing a complete **NLP pipeline**, which is a structured sequence of steps that transforms raw language data into a deployed, working solution.

This lecture explains the **five fundamental steps of an NLP pipeline**, why each step is critical, and how they work together in production-level systems. The instructor strongly emphasizes that **modeling comes only after understanding data, preprocessing, and feature engineering**.

---

## 2. What Is an NLP Pipeline?

An **NLP Pipeline** is a **systematic workflow** that defines how natural language data is:

1. Collected
2. Cleaned and prepared
3. Converted into numerical form
4. Modeled using ML/DL algorithms
5. Deployed and monitored in production

Without a well-designed pipeline, even the most advanced models will fail in real-world applications.

---

## 3. Five Major Steps of an NLP Pipeline

| Step No. | Step Name           | Purpose                           |
| -------- | ------------------- | --------------------------------- |
| 1        | Data Acquisition    | Gather relevant text data         |
| 2        | Text Preparation    | Clean and normalize text          |
| 3        | Feature Engineering | Convert text into numbers         |
| 4        | Modeling            | Train and evaluate models         |
| 5        | Deployment          | Serve, monitor, and update models |

---

## 4. Step-by-Step Explanation with Examples

---

## Step 1: Data Acquisition

### Definition

Data acquisition is the process of **collecting raw textual data** required to solve a specific NLP problem.

ðŸ“Œ **Key Insight:**

> No data â†’ No model â†’ No NLP solution.

---

### Common Data Sources

1. **Internal Data**

   * Company databases
   * CSV / Excel files
   * Customer feedback forms

2. **External Data**

   * Public datasets (Kaggle, UCI)
   * Web scraping (reviews, comments)
   * APIs (Twitter, Reddit, news APIs)

3. **Unstructured Data**

   * PDFs (via OCR)
   * Images (text extraction)
   * Audio (speech-to-text)

---

### Data Augmentation Techniques

Used when data is limited.

ðŸ“Œ **Examples:**

* Synonym replacement

  * â€œGood productâ€ â†’ â€œNice productâ€
* Back translation

  * English â†’ Hindi â†’ English
* Noise injection

  * Minor spelling variations

---

### Mini Example

**Raw reviews:**

```
"This phone is good"
"This phone is nice"
"This phone is excellent"
```

These variations increase dataset size without new manual data collection.

---

## Step 2: Text Preparation (Preprocessing)

### Purpose

Text data is noisy and inconsistent. Preprocessing **standardizes text** so that algorithms can work effectively.

---

### Basic Cleaning Operations

* Remove HTML tags
* Remove special characters
* Normalize Unicode emojis
* Correct spelling mistakes
* Convert to lowercase

ðŸ“Œ **Example:**

Raw text:

```
"I Lâ¤ï¸VE this Product!!! ðŸ˜"
```

After preprocessing:

```
"i love this product"
```

---

### Tokenization

Breaking text into smaller units.

ðŸ“Œ **Example:**
Sentence:

```
"I love NLP"
```

Tokens:

```
["I", "love", "NLP"]
```

---

### Stop Word Removal

Removing common words with little meaning.

ðŸ“Œ **Example:**

```
["I", "am", "learning", "NLP"]
â†’ ["learning", "NLP"]
```

---

### Advanced NLP Processing

* **POS Tagging**
* **Chunking**
* **Co-reference Resolution**

ðŸ“Œ **Example (Co-reference):**

```
"Rahul bought a car. He loves it."
```

Here:

* He â†’ Rahul
* it â†’ car

---

### Important Note

Not all preprocessing steps are mandatory.
The choice depends on:

* Task type
* Model type
* Domain context

---

## Step 3: Feature Engineering

### Definition

Feature engineering is the process of converting **text into numerical representations** that machines can understand.

---

### Why Is Feature Engineering Needed?

ML/DL models **cannot read text directly**.

ðŸ“Œ **Example:**

```
"I love NLP"
```

Must be converted into numbers:

```
[0.2, 0.8, 0.5]
```

---

### Common Feature Techniques

#### 1. Count-Based Features

* Count of positive words
* Count of negative words

ðŸ“Œ **Example:**

| Sentence       | Positive | Negative |
| -------------- | -------- | -------- |
| "Good product" | 1        | 0        |
| "Bad quality"  | 0        | 1        |

---

#### 2. Bag-of-Words / TF-IDF

Represents word frequency importance.

---

#### 3. Word Embeddings

* Word2Vec
* GloVe
* FastText

---

#### 4. Contextual Embeddings

* BERT
* Transformer-based models

---

### ML vs DL in Feature Engineering

| Aspect           | Machine Learning | Deep Learning |
| ---------------- | ---------------- | ------------- |
| Feature creation | Manual           | Automatic     |
| Interpretability | High             | Low           |
| Data requirement | Lowâ€“Medium       | High          |

ðŸ“Œ ML models require **domain expertise**, while DL models learn features automatically.

---

## Step 4: Modeling

### Purpose

Modeling is where **learning happens**.

---

### Types of Models

#### Traditional ML Models

* Logistic Regression
* SVM
* Random Forest

#### Deep Learning Models

* RNN
* LSTM
* Transformers

#### Cloud APIs

* Pretrained NLP services

---

### Evaluation Metrics

* Accuracy
* Confusion Matrix
* Precision / Recall
* Perplexity (text generation)

ðŸ“Œ **Example:**
Spam classifier:

* Accuracy = 95%
* False positives analyzed separately

---

### Transfer Learning

Using pretrained models and fine-tuning.

ðŸ“Œ **Example:**
BERT trained on Wikipedia â†’ fine-tuned for sentiment analysis.

---

## Step 5: Deployment

### Definition

Deployment means **making the model usable by real users**.

---

### Deployment Methods

* REST APIs
* Microservices
* Web applications
* Mobile apps

ðŸ“Œ **Example:**
Spam classifier deployed inside Gmail.

---

### Monitoring

Track:

* Accuracy drift
* User behavior
* Response time

ðŸ“Œ **Example:**
If spam detection accuracy drops, retraining is required.

---

### Model Updating

* Batch retraining
* Online learning
* Periodic updates

Deployment is **continuous**, not a one-time task.

---

## 6. Pipeline Is Not Linear

NLP pipelines often involve **feedback loops**.

ðŸ“Œ **Example:**
Deployment â†’ performance drops â†’ new data â†’ retraining â†’ redeployment

---

## 7. Example Use Case: Sentiment Analysis System

### End-to-End Flow

1. Collect reviews
2. Clean text
3. Extract sentiment features
4. Train classifier
5. Deploy API
6. Monitor feedback

This illustrates the **complete NLP lifecycle**.

---

## 8. Assignment Explained (Quora Duplicate Question Detection)

### Task Objective

Detect whether two questions are semantically identical.

---

### Pipeline Design

* **Data Source:** Quora dataset
* **Preprocessing:** Cleaning, tokenization
* **Features:** Similarity scores, embeddings
* **Model:** ML or transformer
* **Metrics:** Accuracy, F1-score
* **Deployment:** API with monitoring

ðŸ“Œ Focus is on **pipeline thinking**, not just coding.

---

## 9. Summary Table

| Step                | Key Activities         | Tools            |
| ------------------- | ---------------------- | ---------------- |
| Data Acquisition    | Collect & augment data | APIs, scraping   |
| Text Preparation    | Clean & normalize      | NLTK, SpaCy      |
| Feature Engineering | Numeric conversion     | TF-IDF, BERT     |
| Modeling            | Train & evaluate       | ML/DL models     |
| Deployment          | Serve & monitor        | APIs, dashboards |

---

## 10. Final Conclusion

This lecture highlights that **successful NLP systems are built through pipelines, not isolated models**.

### Key Takeaway

> Mastery of the **complete NLP pipelineâ€”from data collection to deploymentâ€”is essential for building scalable, reliable, and production-ready AI solutions.**

The instructor encourages learners to **think like system designers**, not just model trainers.

---
